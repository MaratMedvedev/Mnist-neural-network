{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wv6fmjRarbb-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(0)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "beqt27mP2sOs",
    "outputId": "be52d01f-137b-44b4-dc38-916a2e64c1dd"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7d5a318b3f30>"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data preparation"
   ],
   "metadata": {
    "id": "IbuHzhIb67Wq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "print(y_train[0], X_train.shape)\n",
    "plt.imshow(X_train[0])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "id": "iULXu7VR64N5",
    "outputId": "9c9feaa7-7534-444e-cf80-ae3a0d10fb9f"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5 (60000, 28, 28)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7d590823a260>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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\n"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(y_test[0], X_test.shape)\n",
    "plt.imshow(X_test[0])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "id": "c9Ln63I37UJl",
    "outputId": "2ea4fb40-0b8a-42b5-8494-b1e9ad6d47b7"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "7 (10000, 28, 28)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7d59027be6e0>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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\n"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data reduction"
   ],
   "metadata": {
    "id": "0NSrJLch7axj"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# I will leave only two classes: zeroes and ones\n",
    "train_idxs = np.logical_or(y_train == 1, y_train == 0)\n",
    "X_train = X_train[train_idxs]\n",
    "y_train = y_train[train_idxs]\n",
    "test_idxs = np.logical_or(y_test == 1, y_test == 0)\n",
    "X_test = X_test[test_idxs]\n",
    "y_test = y_test[test_idxs]\n"
   ],
   "metadata": {
    "id": "YwaOaXjU7bIx"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "y_test = y_test.reshape(1, -1)[0]\n",
    "print(\"y_test:\", y_test)\n",
    "X_train = X_train.reshape(-1, 1, 28, 28)\n",
    "X_test = X_test.reshape(-1, 1, 28, 28)\n",
    "X_train= X_train.astype('float64')\n",
    "X_train/=255\n",
    "X_test= X_test.astype('float64')\n",
    "X_test/=255\n",
    "# Needed to reduce the working time\n",
    "X_train = X_train[:2000]\n",
    "y_train = y_train[:2000]\n",
    "X_test = X_test[:400]\n",
    "y_test = y_test[:400]\n",
    "print(\"X_train:\", X_train)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "15ZQkWeR7Nfn",
    "outputId": "3e11b276-b149-483d-f797-cab0afef99ca"
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "y_test: [1 0 1 ... 1 0 1]\n",
      "X_train: [[[[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   ...\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]\n",
      "   [0. 0. 0. ... 0. 0. 0.]]]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# My model"
   ],
   "metadata": {
    "id": "hXkoHLCf644H"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To perform this task, I decided to structure the code in the form of classes for each type of layer, because it help avoid mess in code."
   ],
   "metadata": {
    "id": "EHwRM2dX7G-u"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class Layer:\n",
    "  '''\n",
    "  Abstract class for maintaining the correctness of neural network layers\n",
    "  '''\n",
    "  def forward(self, x, no_grad=False):\n",
    "    raise NotImplementedError(\"Forward method is not implemented!\")\n",
    "\n",
    "  def backward(self, x):\n",
    "    raise NotImplementedError(\"Backward method is not implemented!\")\n",
    "\n",
    "  def step(self, learning_rate: float):\n",
    "    raise NotImplementedError(\"Make step method is not implemented!\")\n",
    "\n",
    "  def zero_grad(self):\n",
    "    raise NotImplementedError(\"Zero grad method is not implemented!\")"
   ],
   "metadata": {
    "id": "3F8vOV8G5N93"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class ReLULayer(Layer):\n",
    "  '''\n",
    "  The ReLU layer doesn't have any parameters because of that step and zero_grad methods do nothing.\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    self.forward_x = np.empty(1)\n",
    "\n",
    "  def forward(self, x, no_grad=False):\n",
    "    if not no_grad:\n",
    "      self.forward_x = np.copy(x)\n",
    "    x[x < 0] = 0\n",
    "    return x\n",
    "\n",
    "  def backward(self, x):\n",
    "    \"\"\"\n",
    "    x: Upstream gradient\n",
    "    Returns downstream gradient\n",
    "    \"\"\"\n",
    "    x[self.forward_x < 0] = 0\n",
    "    return x\n",
    "\n",
    "  def step(self, learning_rate: float):\n",
    "    pass\n",
    "\n",
    "  def zero_grad(self):\n",
    "    pass"
   ],
   "metadata": {
    "id": "HDmM2WAqjenQ"
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class VectorizeLayer(Layer):\n",
    "  '''\n",
    "  The Vectorize layer doesn't have any parameters because of that step and zero_grad methods do nothing.\n",
    "  '''\n",
    "  def forward(self, x, no_grad=False):\n",
    "    if not no_grad:\n",
    "      self.input_shape = x.shape\n",
    "    return x.reshape(-1, 1)\n",
    "\n",
    "  def backward(self, x):\n",
    "    \"\"\"\n",
    "    x: Upstream gradient\n",
    "    Returns downstream gradient\n",
    "    \"\"\"\n",
    "    return x.reshape(self.input_shape)\n",
    "\n",
    "  def step(self, learning_rate: float):\n",
    "    pass\n",
    "\n",
    "  def zero_grad(self):\n",
    "    pass"
   ],
   "metadata": {
    "id": "Xxm9eUdvje8I"
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class NormalizationLayer(Layer):\n",
    "  '''\n",
    "  The Normalization layer doesn't have any parameters because of that step and zero_grad methods do nothing.\n",
    "  '''\n",
    "  def __init__(self):\n",
    "      self.max_abs_value = None\n",
    "\n",
    "  def forward(self, x, no_grad=False):\n",
    "      if not no_grad:\n",
    "        self.max_abs_value = np.max(np.abs(x))\n",
    "      return x / np.max(np.abs(x))\n",
    "\n",
    "  def backward(self, x):\n",
    "      # Since the layer only scales by a constant (max_abs_value), the gradient is just x / max_abs_value\n",
    "      return x / self.max_abs_value\n",
    "\n",
    "  def step(self, learning_rate: float):\n",
    "    pass\n",
    "\n",
    "  def zero_grad(self):\n",
    "    pass"
   ],
   "metadata": {
    "id": "9whSvY_6tecn"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class SoftmaxLayer(Layer):\n",
    "  '''\n",
    "  The Softmax layer doesn't have any parameters because of that step and zero_grad methods do nothing.\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    self.forward_x = None\n",
    "\n",
    "  def forward(self, x, no_grad=False):\n",
    "    if not no_grad:\n",
    "      self.forward_x = x\n",
    "    exp_X = np.exp(x)\n",
    "    return exp_X / np.sum(exp_X)\n",
    "\n",
    "  def backward(self, x):\n",
    "    \"\"\"\n",
    "    x: Upstream gradient\n",
    "    Returns downstream gradient\n",
    "    \"\"\"\n",
    "    dL_dY = x\n",
    "\n",
    "    Y = self.forward(self.forward_x, True)\n",
    "    N = len(Y)\n",
    "    dZ_dX = np.empty([N, N])\n",
    "\n",
    "    # Compute the diagonal and off-diagonal elements of the dZ_dX matrix\n",
    "    for i in range(N):\n",
    "        for j in range(i, N):\n",
    "            # I obtain the following equalities when I manually calculate derivatives.\n",
    "            if i != j:\n",
    "                dZ_dX[i][j] = -Y[i] * Y[j]\n",
    "                dZ_dX[j][i] = dZ_dX[i][j] # Because matrix is symmetric\n",
    "            else:\n",
    "                dZ_dX[i][i] = Y[i] * (1 - Y[i])\n",
    "\n",
    "    # Apply chain rule\n",
    "    dL_dX = np.matmul(dZ_dX, dL_dY)\n",
    "    return dL_dX.reshape(-1, 1)\n",
    "\n",
    "  def step(self, learning_rate: float):\n",
    "    pass\n",
    "\n",
    "  def zero_grad(self):\n",
    "    pass"
   ],
   "metadata": {
    "id": "-bFvMn1Ejw62"
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class ConvolutionalLayer(Layer):\n",
    "\n",
    "  def __init__(self, in_shape, out_channels=3, filter_size=3):\n",
    "      self.in_channels, self.input_height, self.input_width = in_shape\n",
    "      self.out_channels = out_channels\n",
    "      self.filter_size = filter_size\n",
    "      self.output_shape = (out_channels,\n",
    "                           self.input_height - filter_size + 1,\n",
    "                           self.input_width - filter_size + 1\n",
    "                          )\n",
    "\n",
    "      self.filter_grads = np.zeros((out_channels, self.in_channels, filter_size, filter_size))\n",
    "      self.filters = np.random.uniform(-1., 1., (out_channels, self.in_channels, filter_size, filter_size))\n",
    "\n",
    "      self.bias = np.random.uniform(-1., 1., (self.output_shape))\n",
    "      self.bias_grad = np.zeros(self.output_shape)\n",
    "\n",
    "      self.forward_x = None # Indicate the last forwarding value. Need to calculate backward values.\n",
    "      self.n_samples = 0 # Indicate the numbers of forwarding and backwarding samples. Need to calculate mean gradient vector to take step\n",
    "\n",
    "  @staticmethod\n",
    "  def pad(mtr, row_pad, col_pad):\n",
    "    '''\n",
    "    mtr: matrix that will be padded\n",
    "    row_pad: width of padding for row axis\n",
    "    col_pad: width of padding for column axis\n",
    "    '''\n",
    "    if mtr.shape[0] == 1:\n",
    "      mtr = mtr[0]\n",
    "    mtr = np.concatenate((np.zeros((mtr.shape[0], col_pad)), mtr, np.zeros((mtr.shape[0], col_pad))), axis=1)\n",
    "    mtr = np.concatenate((np.zeros((row_pad, mtr.shape[1])), mtr, np.zeros((row_pad, mtr.shape[1]))), axis=0)\n",
    "    mtr = mtr.reshape(1, *mtr.shape)\n",
    "    return mtr\n",
    "\n",
    "  @staticmethod\n",
    "  def rotate_180(mtr):\n",
    "    '''\n",
    "    Rotate matrix by 180 degrees\n",
    "    '''\n",
    "    mtr = mtr[::-1, ::-1]\n",
    "    return mtr\n",
    "\n",
    "  @staticmethod\n",
    "  def conv(X, W):\n",
    "    '''\n",
    "    X: numpy array that represent matrix to which we will apply convolution, using filter W. It has size (D_x, H_x, W_x) - depth, height, width correspondingly\n",
    "    W: numpy array that represent filter that has size (D_w, H_w, W_w) - depth, height, width correspondingly\n",
    "\n",
    "    D_x must be equal to D_w\n",
    "\n",
    "    Output: matrix Z that has size - (1, H_x - H_w + 1, W_x - W_w + 1)\n",
    "    '''\n",
    "    if(len(X.shape) == 2):\n",
    "      X = X.reshape(1, *X.shape)\n",
    "    if(len(W.shape) == 2):\n",
    "      W = W.reshape(1, *W.shape)\n",
    "    D_x, H_x, W_x = X.shape\n",
    "    D_w, H_w, W_w = W.shape\n",
    "    if D_x != D_w:\n",
    "      raise Exception(f\"Depth of the both matrices must be equal, but depth of X equal to {D_x} and depth of W equal to {D_w}\")\n",
    "    H_z = H_x - H_w + 1\n",
    "    W_z = W_x - W_w + 1\n",
    "    Z = np.empty((H_z, W_z))\n",
    "    for i in range(H_z):\n",
    "      for j in range(W_z):\n",
    "        Z[i][j] = np.sum(X[:, i:i+H_w, j:j+W_w] * W)\n",
    "    return Z\n",
    "\n",
    "  def forward(self, x, no_grad=False):\n",
    "    # no_grad indicate that we will consider this x when we will do back propagation or not\n",
    "    # For example, no_grad equal to True needed when we testing our model.\n",
    "    if not no_grad:\n",
    "      self.n_samples += 1\n",
    "      self.forward_x = x\n",
    "    res = np.empty(self.output_shape)\n",
    "    for i in range(self.out_channels):\n",
    "      res[i] = self.conv(x, self.filters[i])\n",
    "    res += self.bias\n",
    "    return res\n",
    "\n",
    "  def backward(self, x):\n",
    "    \"\"\"\n",
    "    x: Upstream gradient\n",
    "    Returns downstream gradient\n",
    "    \"\"\"\n",
    "    self.bias_grad += x # Pass the bias node\n",
    "\n",
    "    # Calculate gradient for filters\n",
    "    for filter_i in range(self.out_channels):\n",
    "      for depth in range(self.in_channels):\n",
    "        self.filter_grads[filter_i][depth] += self.conv(self.forward_x[depth], x[filter_i])\n",
    "\n",
    "    # Calculate the downstream gradient\n",
    "    output_backward = np.zeros((self.in_channels, self.input_height, self.input_width))\n",
    "    for gamma in range(self.in_channels):\n",
    "      for i in range(self.out_channels):\n",
    "        output_backward[gamma] += self.conv(self.pad(x[i], self.filter_size - 1, self.filter_size  - 1), self.rotate_180(self.filters[i][depth]))\n",
    "\n",
    "    return output_backward\n",
    "\n",
    "  def step(self, learning_rate: float):\n",
    "   self.filters -= learning_rate * (self.filter_grads / self.n_samples)\n",
    "   self.bias -= learning_rate * (self.bias_grad / self.n_samples)\n",
    "\n",
    "  def zero_grad(self):\n",
    "    self.filter_grads = np.zeros((self.out_channels, self.in_channels, self.filter_size, self.filter_size))\n",
    "    self.bias_grad = np.zeros(self.output_shape)\n",
    "    self.n_samples = 0\n",
    "    self.forward_x = None"
   ],
   "metadata": {
    "id": "QWkjefn0jeVx"
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class LinearLayer(Layer):\n",
    "  def __init__(self, in_size, out_size):\n",
    "    self.in_size = in_size\n",
    "    self.out_size = out_size\n",
    "    self.W = np.random.uniform(-1., 1., (out_size, in_size))\n",
    "    self.W_grad = np.zeros((out_size, in_size))\n",
    "    self.bias = np.random.uniform(-1., 1., (out_size, 1))\n",
    "    self.bias_grad = np.zeros((out_size, 1))\n",
    "    self.forward_x = None\n",
    "    self.n_samples = 0\n",
    "\n",
    "  def forward(self, x, no_grad=False):\n",
    "    # no_grad indicate that we will consider this x when we will do back propagation or not\n",
    "    # For example, no_grad equal to True needed when we testing our model.\n",
    "    if not no_grad:\n",
    "      self.n_samples += 1\n",
    "      self.forward_x = x\n",
    "    return np.matmul(self.W, x) + self.bias\n",
    "\n",
    "  def backward(self, x):\n",
    "    \"\"\"\n",
    "    x: Upstream gradient\n",
    "    Returns downstream gradient\n",
    "    \"\"\"\n",
    "    # Backprogate through bias node\n",
    "    self.bias_grad += x\n",
    "\n",
    "    # Calculate gradient for W matrix\n",
    "    dL_dW = np.empty(self.W.shape)\n",
    "    n, k = self.W.shape\n",
    "    for i in range(n):\n",
    "      for j in range(k):\n",
    "        dL_dW[i][j] = np.dot(x[i], self.forward_x[j]) # It's equality we can easily get if we multiply correct all corresponding derivatives\n",
    "    self.W_grad += dL_dW\n",
    "\n",
    "    # Calculate downstream gradient\n",
    "    dL_dX = np.empty(self.forward_x.shape)\n",
    "    n, k = self.forward_x.shape\n",
    "    for i in range(n):\n",
    "      for j in range(k):\n",
    "        dL_dX[i][j] = np.dot(x[:, j], self.W[:, i]) # It's equality we can easily get if we multiply correct all corresponding derivatives\n",
    "    return dL_dX\n",
    "\n",
    "  def step(self, learning_rate: float):\n",
    "    self.W -= learning_rate * (self.W_grad / self.n_samples)\n",
    "    self.bias -= learning_rate * (self.bias_grad / self.n_samples)\n",
    "\n",
    "  def zero_grad(self):\n",
    "    self.W_grad = np.zeros(self.W.shape)\n",
    "    self.bias_grad = np.zeros((self.out_size, 1))\n",
    "    self.n_samples = 0\n",
    "    self.forward_x = None"
   ],
   "metadata": {
    "id": "k5c54dqijrdJ"
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class Model:\n",
    "  def __init__(self, layers):\n",
    "    self.layers = layers\n",
    "\n",
    "    # no_grad indicate that we will consider this x when we will do back propagation or not\n",
    "    # For example, no_grad equal to True needed when we testing our model.\n",
    "    self.no_grad = False\n",
    "\n",
    "  def forward(self, x):\n",
    "    for layer in self.layers:\n",
    "      x = layer.forward(x, self.no_grad)\n",
    "    return x\n",
    "\n",
    "  def backward(self, up_stream_gradient):\n",
    "    for layer in self.layers[::-1]:\n",
    "      up_stream_gradient = layer.backward(up_stream_gradient)\n",
    "\n",
    "  def step(self, learning_rate):\n",
    "    for layer in self.layers:\n",
    "      layer.step(learning_rate)\n",
    "\n",
    "  def zero_grad(self):\n",
    "      for layer in self.layers:\n",
    "        layer.zero_grad()\n",
    "\n",
    "  def predict(self, x):\n",
    "    x = self.forward(x)\n",
    "    return np.argmax(x)"
   ],
   "metadata": {
    "id": "52lyV9T5jyXP"
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "num_labels = 2\n",
    "conv1 = ConvolutionalLayer(X_train[0].shape, out_channels=1)\n",
    "conv2 = ConvolutionalLayer(conv1.output_shape, out_channels=1)\n",
    "layers = [\n",
    "         conv1, ReLULayer(),\n",
    "         conv2, ReLULayer(),\n",
    "         VectorizeLayer(),\n",
    "         LinearLayer(np.prod(conv2.output_shape), 10), ReLULayer(),\n",
    "         LinearLayer(10, num_labels), ReLULayer(),\n",
    "         SoftmaxLayer()\n",
    "         ]\n",
    "model = Model(layers)"
   ],
   "metadata": {
    "id": "11u7RQ__cpYL"
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def training(model, X_train, y_train, X_test, y_test, batch_size, learning_rate, epoch):\n",
    "  for i in range(epoch):\n",
    "    cur_idx = 0\n",
    "    print(f\"Epoch {i}:\\n----------------------------\")\n",
    "    batch_N = 0\n",
    "    while len(X_train) - cur_idx >= batch_size:\n",
    "      batch_loss = 0\n",
    "      acc = 0\n",
    "      for i in range(cur_idx, min(cur_idx + batch_size, len(X_train))):\n",
    "        sample = X_train[i]\n",
    "        probs = model.forward(sample)\n",
    "        prediction = np.argmax(probs)\n",
    "\n",
    "        acc += prediction == y_train[i]\n",
    "        batch_loss += -np.log(probs[y_train[i]])\n",
    "\n",
    "        start_backward = np.zeros(len(probs), dtype=float)\n",
    "        start_backward[y_train[i]] = -1 / probs[y_train[i]] # derivative of loss function\n",
    "        model.backward(start_backward)\n",
    "\n",
    "      cur_idx = min(cur_idx + batch_size, len(X_train))\n",
    "      model.step(learning_rate)\n",
    "      model.zero_grad()\n",
    "      print(f\"Training batch {batch_N}:\\n\\t\", end='')\n",
    "      print(\"loss:\", batch_loss / batch_size, \"| accuracy:\", acc / batch_size)\n",
    "      batch_N += 1\n",
    "      # print(\"Validation:\\n\\t\", end='')\n",
    "      # testing(model, X_test, y_test)\n",
    "\n",
    "def testing(model, X_test, y_test):\n",
    "  acc = 0\n",
    "  avg_loss = 0\n",
    "  model.no_grad = True\n",
    "  for i, sample in enumerate(X_test):\n",
    "    probs = model.forward(sample)\n",
    "    avg_loss += -np.log(probs[y_test[i]])\n",
    "    acc += np.argmax(probs) == y_test[i]\n",
    "  model.no_grad = False\n",
    "  print(\"loss:\", avg_loss / len(X_test), \"| accuracy:\", acc / len(X_test))"
   ],
   "metadata": {
    "id": "3oOh113rcqJS"
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "batch_size=80\n",
    "learning_rate=0.01\n",
    "epoch = 2"
   ],
   "metadata": {
    "id": "P9JmxVeQjWYD"
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "training(model, X_train, y_train, X_test, y_test, batch_size=batch_size, learning_rate=learning_rate, epoch=epoch)"
   ],
   "metadata": {
    "id": "7wX8vbp76ftA",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "4dfa03a0-6899-4873-c585-13271e80a699"
   },
   "execution_count": 18,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0:\n",
      "----------------------------\n",
      "Training batch 0:\n",
      "\tloss: [3.4984845] | accuracy: 0.55\n",
      "Training batch 1:\n",
      "\tloss: [1.45501513] | accuracy: 0.55\n",
      "Training batch 2:\n",
      "\tloss: [0.65006088] | accuracy: 0.65\n",
      "Training batch 3:\n",
      "\tloss: [0.77687364] | accuracy: 0.5875\n",
      "Training batch 4:\n",
      "\tloss: [0.73136194] | accuracy: 0.625\n",
      "Training batch 5:\n",
      "\tloss: [0.75558968] | accuracy: 0.6875\n",
      "Training batch 6:\n",
      "\tloss: [0.64017649] | accuracy: 0.6625\n",
      "Training batch 7:\n",
      "\tloss: [0.6859961] | accuracy: 0.575\n",
      "Training batch 8:\n",
      "\tloss: [0.52210708] | accuracy: 0.775\n",
      "Training batch 9:\n",
      "\tloss: [0.57417589] | accuracy: 0.7125\n",
      "Training batch 10:\n",
      "\tloss: [0.56735528] | accuracy: 0.7125\n",
      "Training batch 11:\n",
      "\tloss: [0.62525991] | accuracy: 0.6875\n",
      "Training batch 12:\n",
      "\tloss: [0.62082444] | accuracy: 0.6875\n",
      "Training batch 13:\n",
      "\tloss: [0.60324726] | accuracy: 0.7125\n",
      "Training batch 14:\n",
      "\tloss: [0.50675374] | accuracy: 0.775\n",
      "Training batch 15:\n",
      "\tloss: [0.52482563] | accuracy: 0.75\n",
      "Training batch 16:\n",
      "\tloss: [0.46319969] | accuracy: 0.8375\n",
      "Training batch 17:\n",
      "\tloss: [0.52235894] | accuracy: 0.75\n",
      "Training batch 18:\n",
      "\tloss: [0.42215806] | accuracy: 0.875\n",
      "Training batch 19:\n",
      "\tloss: [0.53032445] | accuracy: 0.7375\n",
      "Training batch 20:\n",
      "\tloss: [0.48522679] | accuracy: 0.8125\n",
      "Training batch 21:\n",
      "\tloss: [0.46375087] | accuracy: 0.85\n",
      "Training batch 22:\n",
      "\tloss: [0.42912307] | accuracy: 0.8\n",
      "Training batch 23:\n",
      "\tloss: [0.34112732] | accuracy: 0.8875\n",
      "Training batch 24:\n",
      "\tloss: [0.34358405] | accuracy: 0.8375\n",
      "Epoch 1:\n",
      "----------------------------\n",
      "Training batch 0:\n",
      "\tloss: [0.43770671] | accuracy: 0.775\n",
      "Training batch 1:\n",
      "\tloss: [0.47091357] | accuracy: 0.85\n",
      "Training batch 2:\n",
      "\tloss: [0.32895625] | accuracy: 0.9125\n",
      "Training batch 3:\n",
      "\tloss: [0.39925874] | accuracy: 0.875\n",
      "Training batch 4:\n",
      "\tloss: [0.35477953] | accuracy: 0.875\n",
      "Training batch 5:\n",
      "\tloss: [0.38302708] | accuracy: 0.875\n",
      "Training batch 6:\n",
      "\tloss: [0.33568345] | accuracy: 0.9\n",
      "Training batch 7:\n",
      "\tloss: [0.44973255] | accuracy: 0.8\n",
      "Training batch 8:\n",
      "\tloss: [0.2587575] | accuracy: 0.925\n",
      "Training batch 9:\n",
      "\tloss: [0.30778877] | accuracy: 0.9375\n",
      "Training batch 10:\n",
      "\tloss: [0.28185109] | accuracy: 0.925\n",
      "Training batch 11:\n",
      "\tloss: [0.30256087] | accuracy: 0.8875\n",
      "Training batch 12:\n",
      "\tloss: [0.34019019] | accuracy: 0.8625\n",
      "Training batch 13:\n",
      "\tloss: [0.38344929] | accuracy: 0.85\n",
      "Training batch 14:\n",
      "\tloss: [0.30543941] | accuracy: 0.9\n",
      "Training batch 15:\n",
      "\tloss: [0.3284545] | accuracy: 0.925\n",
      "Training batch 16:\n",
      "\tloss: [0.26118713] | accuracy: 0.9625\n",
      "Training batch 17:\n",
      "\tloss: [0.33187517] | accuracy: 0.8875\n",
      "Training batch 18:\n",
      "\tloss: [0.2694129] | accuracy: 0.925\n",
      "Training batch 19:\n",
      "\tloss: [0.35565299] | accuracy: 0.875\n",
      "Training batch 20:\n",
      "\tloss: [0.33498377] | accuracy: 0.875\n",
      "Training batch 21:\n",
      "\tloss: [0.25812641] | accuracy: 0.9125\n",
      "Training batch 22:\n",
      "\tloss: [0.25022689] | accuracy: 0.925\n",
      "Training batch 23:\n",
      "\tloss: [0.19149233] | accuracy: 0.95\n",
      "Training batch 24:\n",
      "\tloss: [0.22955121] | accuracy: 0.925\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "testing(model, X_test, y_test)"
   ],
   "metadata": {
    "id": "P5TiSF5RjeBq",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "63cd6603-6e10-4b65-c5a6-fad2986c0fef"
   },
   "execution_count": 19,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loss: [0.23642654] | accuracy: 0.935\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The same pytorch model"
   ],
   "metadata": {
    "id": "lgU81d_PsFXv"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Unfortunately, pytoch conv2d layer bias term logic doesn't works like convolution node bias term that was I implemented, because size of  pytorch conv2d bias term is num_channels x 1 x 1(For each channel, it add one number that corresponded to this channel). So, to check that forward and backward propagation work identical I make zero all convolutional bias terms for both models: for mine and torch model. And then I compare results of forward propagation, and backward propagation for all parameters of both models."
   ],
   "metadata": {
    "id": "OFirWbEnq2uA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TorchModel(nn.Module):\n",
    "    def __init__(self, num_labels, conv1_weight, conv2_weight, linear1_weight, linear1_bias, linear2_weight, linear2_bias):\n",
    "        super(TorchModel, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, padding=0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, padding=0)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.vectorize = nn.Flatten()\n",
    "        self.linear1 = nn.Linear(24*24, 10)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(10, num_labels)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        # Set my own weights and biases for each layer\n",
    "\n",
    "        self.conv1.weight = nn.Parameter(conv1_weight)\n",
    "        self.conv1.bias = nn.Parameter(torch.zeros(self.conv2.bias.shape).double())\n",
    "        self.conv2.weight = nn.Parameter(conv2_weight)\n",
    "        self.conv2.bias = nn.Parameter(torch.zeros(self.conv2.bias.shape).double())\n",
    "\n",
    "        self.linear1.weight = nn.Parameter(linear1_weight)\n",
    "        self.linear1.bias = nn.Parameter(linear1_bias.reshape(1, 10))\n",
    "\n",
    "        self.linear2.weight = nn.Parameter(linear2_weight)\n",
    "        self.linear2.bias = nn.Parameter(linear2_bias.reshape(1, 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.vectorize(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n"
   ],
   "metadata": {
    "id": "xOOrxdgDjj64"
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Checking  that the pytorch model and mine are identical"
   ],
   "metadata": {
    "id": "houbtmGfyGIT"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preparing our pytorch model\n",
    "We have to make the parameters the same for pytorch model and mine\n"
   ],
   "metadata": {
    "id": "7PZ_x9GFzQQG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "x = X_train[0]\n",
    "y = y_train[0]\n",
    "conv1_weight = torch.from_numpy(model.layers[0].filters.astype(np.float64))\n",
    "model.layers[0].bias.fill(0.)\n",
    "conv2_weight = torch.from_numpy(model.layers[2].filters.astype(np.float64))\n",
    "model.layers[2].bias.fill(0.)\n",
    "linear1_weight = torch.from_numpy(model.layers[5].W.astype(np.float64))\n",
    "linear1_bias = torch.from_numpy(model.layers[5].bias.astype(np.float64))\n",
    "linear2_weight = torch.from_numpy(model.layers[7].W.astype(np.float64))\n",
    "linear2_bias = torch.from_numpy(model.layers[7].bias.astype(np.float64))\n",
    "\n",
    "torch_model = TorchModel(num_labels, conv1_weight, conv2_weight, linear1_weight, linear1_bias, linear2_weight, linear2_bias)"
   ],
   "metadata": {
    "id": "xM2VWdy0aNs6"
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Checking that forward propagation is the same"
   ],
   "metadata": {
    "id": "ZGCJ7DQxyl9R"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "torch_output = torch_model.forward(torch.from_numpy(x).to(torch.float64))[0]\n",
    "\n",
    "my_output = model.forward(x)\n",
    "mae = (np.abs(torch_output.detach().numpy() - my_output.T)).mean()\n",
    "print(\"MAE for mine forward propagation output and pytorch output:\", mae)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lmwUdZSGbKv7",
    "outputId": "6b64931f-931f-4f7e-9951-f0010fcab3be"
   },
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MAE for mine forward propagation output and pytorch output: 2.7755575615628914e-17\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Checking that backward propagation is same for both models\n",
    "To be sure about this, I just compare all learnable parameters for both models. For that purpose, I use mean absolute error.\n"
   ],
   "metadata": {
    "id": "tsrjjWqCyyrV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "target = torch.zeros(torch_output.shape)\n",
    "target[y] = 1.\n",
    "loss = torch.sum(-torch.log(torch_output) * target)\n",
    "\n",
    "loss.backward()\n",
    "# Calculate backward for loss and do back propagation for my model\n",
    "start_backward = target.numpy()\n",
    "start_backward /= -my_output[y] # loss derivative\n",
    "start_backward = start_backward.reshape(-1, 1)\n",
    "model.backward(start_backward)\n",
    "\n",
    "conv1_weight_grad = torch_model.conv1.weight.grad\n",
    "print(\"MAE for first convolutional layer parameters:\", np.sum(np.abs(conv1_weight_grad.numpy() - model.layers[0].filter_grads)))\n",
    "\n",
    "conv2_weight_grad = torch_model.conv2.weight.grad\n",
    "print(\"MAE for second convolutional layer parameters:\", np.sum(np.abs(conv2_weight_grad.numpy() - model.layers[2].filter_grads)))\n",
    "\n",
    "linear1_weight_grad = torch_model.linear1.weight.grad\n",
    "linear1_bias_grad = torch_model.linear1.bias.grad\n",
    "\n",
    "print(\"MAE for first linear layer parameters:\",\n",
    "      np.sum(np.abs(linear1_weight_grad.numpy() - model.layers[5].W_grad))\n",
    "      + np.sum(np.abs(linear1_bias_grad.numpy() - model.layers[5].bias_grad.T)))\n",
    "\n",
    "linear2_weight_grad = torch_model.linear2.weight.grad\n",
    "linear2_bias_grad = torch_model.linear2.bias.grad\n",
    "\n",
    "print(\"MAE for second linear layer parameters:\",\n",
    "      np.sum(np.abs(linear2_weight_grad.numpy() - model.layers[7].W_grad))\n",
    "      + np.sum(np.abs(linear2_bias_grad.numpy() - model.layers[7].bias_grad.T)))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TuaVeLxHyyXV",
    "outputId": "5437dff9-3c3e-450e-996f-8b71bbacaf0b"
   },
   "execution_count": 23,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MAE for first convolutional layer parameters: 1.439207898967787e-07\n",
      "MAE for second convolutional layer parameters: 3.0218244061967076e-08\n",
      "MAE for first linear layer parameters: 1.832702355018951e-07\n",
      "MAE for second linear layer parameters: 7.779263647145207e-08\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pytorch model training"
   ],
   "metadata": {
    "id": "oec20-Wy326v"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def my_loss_fn(output, target):\n",
    "  targets = torch.zeros(output.shape)\n",
    "  for i, label in enumerate(target):\n",
    "    targets[i][label.item()] = 1.\n",
    "  return torch.sum(-torch.log(output) * targets, dim=1).mean()\n",
    "\n",
    "def testing(torch_model, X_test, y_test, loss_fn):\n",
    "  with torch.no_grad():\n",
    "    outputs = torch_model(torch.from_numpy(X_test).to(torch.float64))\n",
    "    labels = torch.from_numpy(y_test)\n",
    "    loss = loss_fn(outputs, labels)\n",
    "    acc = (torch.argmax(outputs, dim=1) == labels).float().mean().item()\n",
    "    print(f\"loss: {loss.item()} | accuracy: {acc}\")\n",
    "\n",
    "def training_torch_model(torch_model, X_train, y_train, epoch, optimizer, loss_fn, batch_size):\n",
    "    for i in range(epoch):\n",
    "      print(f\"Epoch {i}:\\n----------------------------\")\n",
    "      batch_N = 0\n",
    "      optimizer.zero_grad()\n",
    "      for j in range(0, len(X_train), batch_size):\n",
    "        if(j + batch_size > len(X_train)):\n",
    "          break\n",
    "        l = j\n",
    "        r = l+batch_size\n",
    "        inputs = X_train[l:r]\n",
    "        labels = torch.from_numpy(y_train[l:r])\n",
    "        acc = 0\n",
    "        s_loss = 0\n",
    "        outputs = torch_model(torch.from_numpy(inputs).to(torch.float64))\n",
    "\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        acc = (torch.argmax(outputs, dim=1) == labels).float().mean().item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        print(f\"Training batch {batch_N}:\\n\\t\", end='')\n",
    "        print(\"loss:\", loss.item(), \"| accuracy:\", acc)\n",
    "        batch_N += 1\n"
   ],
   "metadata": {
    "id": "2vIMo-q3376q"
   },
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "optimizer = torch.optim.SGD(torch_model.parameters(), lr=learning_rate)\n",
    "loss = my_loss_fn"
   ],
   "metadata": {
    "id": "Pk7WGTzK8NTS"
   },
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "training_torch_model(torch_model, X_train, y_train, epoch=epoch, optimizer=optimizer, loss_fn=loss, batch_size=batch_size)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bhtENat8UvgP",
    "outputId": "47006630-b965-4524-ec24-02db51e04b3b"
   },
   "execution_count": 26,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0:\n",
      "----------------------------\n",
      "Training batch 0:\n",
      "\tloss: 0.7759614281728939 | accuracy: 0.44999998807907104\n",
      "Training batch 1:\n",
      "\tloss: 0.8492774175844744 | accuracy: 0.36250001192092896\n",
      "Training batch 2:\n",
      "\tloss: 0.8073144462341535 | accuracy: 0.4749999940395355\n",
      "Training batch 3:\n",
      "\tloss: 0.7789486670546051 | accuracy: 0.4625000059604645\n",
      "Training batch 4:\n",
      "\tloss: 0.7340277986089876 | accuracy: 0.5249999761581421\n",
      "Training batch 5:\n",
      "\tloss: 0.7654018157351089 | accuracy: 0.4625000059604645\n",
      "Training batch 6:\n",
      "\tloss: 0.7945547807221038 | accuracy: 0.44999998807907104\n",
      "Training batch 7:\n",
      "\tloss: 0.7805253225717206 | accuracy: 0.4124999940395355\n",
      "Training batch 8:\n",
      "\tloss: 0.7521027499042798 | accuracy: 0.4000000059604645\n",
      "Training batch 9:\n",
      "\tloss: 0.7115456999650265 | accuracy: 0.4375\n",
      "Training batch 10:\n",
      "\tloss: 0.7758480921034661 | accuracy: 0.4375\n",
      "Training batch 11:\n",
      "\tloss: 0.7358718170217287 | accuracy: 0.4625000059604645\n",
      "Training batch 12:\n",
      "\tloss: 0.7014662896394807 | accuracy: 0.5375000238418579\n",
      "Training batch 13:\n",
      "\tloss: 0.639409577851986 | accuracy: 0.6000000238418579\n",
      "Training batch 14:\n",
      "\tloss: 0.6452785354514852 | accuracy: 0.6499999761581421\n",
      "Training batch 15:\n",
      "\tloss: 0.6429309099170235 | accuracy: 0.625\n",
      "Training batch 16:\n",
      "\tloss: 0.5524301791427505 | accuracy: 0.8125\n",
      "Training batch 17:\n",
      "\tloss: 0.621001556867743 | accuracy: 0.75\n",
      "Training batch 18:\n",
      "\tloss: 0.6139548116434247 | accuracy: 0.7749999761581421\n",
      "Training batch 19:\n",
      "\tloss: 0.5457948960773631 | accuracy: 0.8374999761581421\n",
      "Training batch 20:\n",
      "\tloss: 0.5713323216195599 | accuracy: 0.737500011920929\n",
      "Training batch 21:\n",
      "\tloss: 0.5376976777082458 | accuracy: 0.8125\n",
      "Training batch 22:\n",
      "\tloss: 0.5487156368956255 | accuracy: 0.7875000238418579\n",
      "Training batch 23:\n",
      "\tloss: 0.48770059910707975 | accuracy: 0.862500011920929\n",
      "Training batch 24:\n",
      "\tloss: 0.4548148993133774 | accuracy: 0.8374999761581421\n",
      "Epoch 1:\n",
      "----------------------------\n",
      "Training batch 0:\n",
      "\tloss: 0.5257523827062034 | accuracy: 0.737500011920929\n",
      "Training batch 1:\n",
      "\tloss: 0.5379631452283167 | accuracy: 0.75\n",
      "Training batch 2:\n",
      "\tloss: 0.5271391418439425 | accuracy: 0.7749999761581421\n",
      "Training batch 3:\n",
      "\tloss: 0.48238761764188853 | accuracy: 0.824999988079071\n",
      "Training batch 4:\n",
      "\tloss: 0.4895692512668462 | accuracy: 0.7875000238418579\n",
      "Training batch 5:\n",
      "\tloss: 0.5354773174042193 | accuracy: 0.75\n",
      "Training batch 6:\n",
      "\tloss: 0.4311472867909007 | accuracy: 0.8500000238418579\n",
      "Training batch 7:\n",
      "\tloss: 0.48706824802731574 | accuracy: 0.824999988079071\n",
      "Training batch 8:\n",
      "\tloss: 0.43159948593942354 | accuracy: 0.8374999761581421\n",
      "Training batch 9:\n",
      "\tloss: 0.5667363008208829 | accuracy: 0.699999988079071\n",
      "Training batch 10:\n",
      "\tloss: 0.49390469918722174 | accuracy: 0.762499988079071\n",
      "Training batch 11:\n",
      "\tloss: 0.49131934573158775 | accuracy: 0.7875000238418579\n",
      "Training batch 12:\n",
      "\tloss: 0.442610801965906 | accuracy: 0.800000011920929\n",
      "Training batch 13:\n",
      "\tloss: 0.45755030561239324 | accuracy: 0.862500011920929\n",
      "Training batch 14:\n",
      "\tloss: 0.47112505180825054 | accuracy: 0.824999988079071\n",
      "Training batch 15:\n",
      "\tloss: 0.4532580366027312 | accuracy: 0.800000011920929\n",
      "Training batch 16:\n",
      "\tloss: 0.41270312944501975 | accuracy: 0.8500000238418579\n",
      "Training batch 17:\n",
      "\tloss: 0.48270345502244305 | accuracy: 0.7875000238418579\n",
      "Training batch 18:\n",
      "\tloss: 0.46705159027883597 | accuracy: 0.7749999761581421\n",
      "Training batch 19:\n",
      "\tloss: 0.43081679144685464 | accuracy: 0.824999988079071\n",
      "Training batch 20:\n",
      "\tloss: 0.476316217908524 | accuracy: 0.8125\n",
      "Training batch 21:\n",
      "\tloss: 0.436953936755111 | accuracy: 0.8125\n",
      "Training batch 22:\n",
      "\tloss: 0.46641264850960373 | accuracy: 0.7875000238418579\n",
      "Training batch 23:\n",
      "\tloss: 0.3753625974679108 | accuracy: 0.8374999761581421\n",
      "Training batch 24:\n",
      "\tloss: 0.3771478503329224 | accuracy: 0.8999999761581421\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "testing(torch_model, X_test, y_test, loss_fn=loss)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7UQ1LuOfUwLU",
    "outputId": "5e8b7084-4a97-4afc-9b65-9dbf4f110d75"
   },
   "execution_count": 27,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loss: 0.4134043811923756 | accuracy: 0.8149999976158142\n"
     ]
    }
   ]
  }
 ]
}
